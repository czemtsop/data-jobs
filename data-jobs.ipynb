{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/caddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/caddy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/caddy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # For data manipulation and creating DataFrames\n",
    "import requests  # For making HTTP requests to the API\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords as STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "stopwords = set(STOPWORDS.words('english'))  # Load the English stopwords from NLTK\n",
    "stopwords.update(['across', 'help', 'skills', 'will'])  # Add custom stopwords\n",
    "\n",
    "# Define the stopwords for text processing\n",
    "\n",
    "\n",
    "def fetch_jobs_from_api(sites):\n",
    "    \"\"\"\n",
    "    Fetches job listings from the APIs of the specified sites.\n",
    "\n",
    "    Args:\n",
    "        sites (list): A list of site names to fetch jobs from. Valid options are 'remoteok' and 'jobicy'.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe of job listings if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    api_urls = {\n",
    "        \"remoteok\": \"https://remoteok.com/api\",\n",
    "        'jobicy': \"https://jobicy.com/api/v2/remote-jobs\"\n",
    "    }\n",
    "    \n",
    "    for site in sites:\n",
    "        if site not in api_urls:\n",
    "            print(f\"Error: {site} is not a valid site. Valid sites are: {', '.join(api_urls.keys())}\")\n",
    "            return None\n",
    "        api_url = api_urls[site]\n",
    "        print(f\"Attempting to fetch data from: {api_url}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, timeout=10)  # Added timeout for robustness\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "\n",
    "            data = response.json()\n",
    "            if (isinstance(data, list) or isinstance(data, dict)) and len(data) > 0:\n",
    "                if site == 'remoteok':\n",
    "                    return parse_remoteok_jobs_to_structured_df(data)\n",
    "                else:\n",
    "                    return parse_jobicy_jobs_to_structured_df(data)\n",
    "\n",
    "            elif (isinstance(data, list) or isinstance(data, dict)) and len(data) == 0:\n",
    "                print(\"API returned an empty list of jobs.\")\n",
    "                return []\n",
    "            else:\n",
    "                print(f\"Unexpected API response format. Expected a list, got {type(data)}.\")\n",
    "                return None\n",
    "\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"Error: Connection error occurred while trying to reach {api_url}.\")\n",
    "            return None\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(f\"Error: Too many redirects while trying to reach {api_url}.\")\n",
    "            return None        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error: Request to {api_url} timed out.\")\n",
    "            return None\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            print(f\"Error: HTTP error occurred: {http_err} - Status Code: {response.status_code}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as req_err:\n",
    "            print(f\"Error: An error occurred while fetching data from API: {req_err}\")\n",
    "            return None\n",
    "        except ValueError as json_err:  # Includes json.JSONDecodeError\n",
    "            print(f\"Error: Could not decode JSON response: {json_err}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def parse_remoteok_jobs_to_structured_df(data):\n",
    "    \"\"\"\n",
    "    Parses a list of job dictionaries (from API) into a pandas DataFrame.\n",
    "    Selects relevant columns and performs basic data cleaning/transformation.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries from the API.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing structured job data, or an empty DataFrame if input is invalid.\n",
    "    \"\"\"\n",
    "    # The RemoteOK API returns a list. The first item is a \"legal notice\" or API info.\n",
    "    # Actual job listings start from the second item.\n",
    "    if data[0].get(\"legal\") is not None:\n",
    "        print(f\"Skipping the first element (meta-data/legal): {data[0].get('legal')}\")\n",
    "        job_list = data[1:]\n",
    "    else:\n",
    "        # If the first element doesn't look like metadata, perhaps the API structure changed.\n",
    "        # For now, we'll assume it's all job data.\n",
    "        print(\"First element does not appear to be metadata. Processing all elements as jobs.\")\n",
    "        job_list = data\n",
    "    \n",
    "    if not job_list:\n",
    "        print(\"No job data provided. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Normalizing {len(job_list)} job entries into a DataFrame...\")\n",
    "    # Use pandas.json_normalize to flatten the JSON structures.\n",
    "    df = pd.json_normalize(job_list)\n",
    "\n",
    "    # --- Data Cleaning and Transformation ---\n",
    "\n",
    "    # Define the columns we are interested in.\n",
    "    desired_columns = [\n",
    "        'id', 'company', 'position', 'tags', 'location', 'salary_min', 'salary_max'\n",
    "    ]\n",
    "    # Define the keywords to look for in job titles\n",
    "    keywords = 'analy|data|machine learning|intelligence'\n",
    "\n",
    "    # Select only the desired columns that are actually present in the DataFrame\n",
    "    # This makes the script more robust to changes in the API response\n",
    "    columns_to_select = [col for col in desired_columns if col in df.columns]\n",
    "\n",
    "    if not columns_to_select:\n",
    "        print(\"None of the desired columns were found in the API response. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_selected = df[columns_to_select][df['position'].str.contains(keywords, case=False) |\n",
    "                                        df['tags'].str.contains(keywords,\n",
    "                                                                case=False)].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Convert 'epoch' to datetime objects\n",
    "    if 'epoch' in df.columns:\n",
    "        # Ensure 'epoch' is numeric, coercing errors to NaT (Not a Time)\n",
    "        df_selected['epoch'] = pd.to_datetime(df['epoch'], unit='s', errors='coerce')\n",
    "\n",
    "    # Convert 'tags' list into a comma-separated string for easier use in SQL/CSV.\n",
    "    if 'tags' in df_selected.columns:\n",
    "        df_selected['tags_string'] = df_selected['tags'].apply(\n",
    "            lambda tags_list: ', '.join(tags_list) if isinstance(tags_list, list) and tags_list else None\n",
    "        )\n",
    "\n",
    "    # Clean up HTML and robot message from description\n",
    "    if 'description' in df.columns:\n",
    "        df_selected['description'] = df['description'].apply(\n",
    "            lambda html: bs(html, 'html.parser').get_text()\n",
    "        )\n",
    "        df_selected['description'] = df_selected['description'].str.replace(r'Please mention the word(.)*', \"\",\n",
    "                                                                            regex=True)\n",
    "\n",
    "    df_selected['source'] = 'REmote OK'\n",
    "\n",
    "    return df_selected\n",
    "\n",
    "\n",
    "def parse_jobicy_jobs_to_structured_df(data):\n",
    "    \"\"\"\n",
    "    Parses a job dictionary (from API) into a pandas DataFrame.\n",
    "    Selects relevant columns and performs basic data cleaning/transformation.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary from the API.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing structured job data, or an empty DataFrame if input is invalid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The jobicy API returns a list with metadata.\n",
    "    # Actual job listings are in the \"job\" dictionary.\n",
    "    print(f\"Friendly notice: {data.get('friendlyNotice')}\")\n",
    "    job_list = data.get('jobs', [])\n",
    "    \n",
    "    if not job_list or not isinstance(job_list, list):\n",
    "        print(\"No job data provided or data is not in list format. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Normalizing {len(job_list)} job entries into a DataFrame...\")\n",
    "    # Use pandas.json_normalize to flatten the JSON structures.\n",
    "    df = pd.json_normalize(job_list)\n",
    "\n",
    "    # --- Data Cleaning and Transformation ---\n",
    "\n",
    "    # Define the columns we are interested in.\n",
    "    desired_columns = [\n",
    "        'id', 'company', 'position', 'tags', 'location', 'salary_min', 'salary_max'\n",
    "    ]\n",
    "    # Define the keywords to look for in job titles\n",
    "    keywords = 'analy|data|machine learning|intelligence'\n",
    "\n",
    "    # Select only the desired columns that are actually present in the DataFrame\n",
    "    # This makes the script more robust to changes in the API response\n",
    "    columns_to_select = [col for col in desired_columns if col in df.columns]\n",
    "\n",
    "    if not columns_to_select:\n",
    "        print(\"None of the desired columns were found in the API response. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_selected = df[columns_to_select][df['position'].str.contains(keywords, case=False) |\n",
    "                                        df['tags'].str.contains(keywords,\n",
    "                                                                case=False)].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Convert 'epoch' to datetime objects\n",
    "    if 'epoch' in df.columns:\n",
    "        # Ensure 'epoch' is numeric, coercing errors to NaT (Not a Time)\n",
    "        df_selected['epoch'] = pd.to_datetime(df['epoch'], unit='s', errors='coerce')\n",
    "\n",
    "    # Convert 'tags' list into a comma-separated string for easier use in SQL/CSV.\n",
    "    if 'tags' in df_selected.columns:\n",
    "        df_selected['tags_string'] = df_selected['tags'].apply(\n",
    "            lambda tags_list: ', '.join(tags_list) if isinstance(tags_list, list) and tags_list else None\n",
    "        )\n",
    "\n",
    "    # Clean up HTML and robot message from description\n",
    "    if 'description' in df.columns:\n",
    "        df_selected['description'] = df['description'].apply(\n",
    "            lambda html: bs(html, 'html.parser').get_text()\n",
    "        )\n",
    "        df_selected['description'] = df_selected['description'].str.replace(r'Please mention the word(.)*', \"\",\n",
    "                                                                            regex=True)\n",
    "\n",
    "    df_selected['source'] = 'REmote OK'\n",
    "\n",
    "    return df_selected\n",
    "\n",
    "\n",
    "def generate_wordcloud(text, mask_image_path=None):\n",
    "    \"\"\"\n",
    "    Generates a word cloud from the provided text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to generate the word cloud from.\n",
    "        mask_image_path (str): Path to an image file to use as a mask for the word cloud.\n",
    "\n",
    "    Returns:\n",
    "        WordCloud: A WordCloud object.\n",
    "    \"\"\"\n",
    "    if mask_image_path and path.exists(mask_image_path):\n",
    "        mask = Image.open(mask_image_path)\n",
    "        mask = mask.convert(\"L\")  # Convert to grayscale\n",
    "        mask_array = np.array(mask)\n",
    "    else:\n",
    "        mask_array = None\n",
    "\n",
    "    wc = WordCloud(width=800, height=400,\n",
    "                   background_color='white',\n",
    "                   max_words=30,\n",
    "                   stopwords=stopwords,\n",
    "                   mask=mask_array,\n",
    "                   contour_color='steelblue',\n",
    "                   contour_width=1).generate(text)\n",
    "\n",
    "    return wc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaccd128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://remoteok.com/api\n",
      "Skipping the first element (meta-data/legal): API Terms of Service: Please link back (with follow, and without nofollow!) to the URL on Remote OK and mention Remote OK as a source, so we get traffic back from your site. If you do not we'll have to suspend API access.\n",
      "\n",
      "Please don't use the Remote OK logo without written permission as it's a registered trademark, please DO use our name Remote OK though.\n",
      "Normalizing 95 job entries into a DataFrame...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>position</th>\n",
       "      <th>tags</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>epoch</th>\n",
       "      <th>tags_string</th>\n",
       "      <th>description</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1093192</td>\n",
       "      <td>Ironclad</td>\n",
       "      <td>Manager Engineering II Data Pipelines</td>\n",
       "      <td>[manager, design, hr, docker, technical, softw...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>67500</td>\n",
       "      <td>97500</td>\n",
       "      <td>2025-05-17 13:00:04</td>\n",
       "      <td>manager, design, hr, docker, technical, softwa...</td>\n",
       "      <td>Ironclad is the #1 contract lifecycle manageme...</td>\n",
       "      <td>REmote OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1093176</td>\n",
       "      <td>Warner Music Inc.</td>\n",
       "      <td>Staff Machine Learning Engineer</td>\n",
       "      <td>[music, design, system, training, software, cl...</td>\n",
       "      <td></td>\n",
       "      <td>60000</td>\n",
       "      <td>97500</td>\n",
       "      <td>2025-05-15 00:00:20</td>\n",
       "      <td>music, design, system, training, software, clo...</td>\n",
       "      <td>At Warner Music Group, weâre a global collec...</td>\n",
       "      <td>REmote OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1093152</td>\n",
       "      <td>Fullscript</td>\n",
       "      <td>Senior Data Analytics Engineer</td>\n",
       "      <td>[design, python, technical, support, testing, ...</td>\n",
       "      <td></td>\n",
       "      <td>67500</td>\n",
       "      <td>120000</td>\n",
       "      <td>2025-05-10 21:00:03</td>\n",
       "      <td>design, python, technical, support, testing, g...</td>\n",
       "      <td>At Fullscript, weâre not just changing healt...</td>\n",
       "      <td>REmote OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1093124</td>\n",
       "      <td>Utility Profit + Sunroom</td>\n",
       "      <td>Data and Business Intelligence Engineer Mid Le...</td>\n",
       "      <td>[design, saas, leader, operations, telecom, en...</td>\n",
       "      <td></td>\n",
       "      <td>70000</td>\n",
       "      <td>120000</td>\n",
       "      <td>2025-05-07 08:00:15</td>\n",
       "      <td>design, saas, leader, operations, telecom, eng...</td>\n",
       "      <td>About Utility ProfitUtility Profit is transfor...</td>\n",
       "      <td>REmote OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1093098</td>\n",
       "      <td>500 WP Company LLC</td>\n",
       "      <td>Computational Journalist Data Reporting</td>\n",
       "      <td>[embedded, training, software, director, devop...</td>\n",
       "      <td>DC-Washington-TWP Headquarters</td>\n",
       "      <td>57500</td>\n",
       "      <td>80000</td>\n",
       "      <td>2025-05-03 00:00:03</td>\n",
       "      <td>embedded, training, software, director, devops...</td>\n",
       "      <td>Application Instructions Please list all profe...</td>\n",
       "      <td>REmote OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1093062</td>\n",
       "      <td>KPA</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>[analyst, saas, salesforce, training, consulti...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>70000</td>\n",
       "      <td>115000</td>\n",
       "      <td>2025-04-26 16:00:02</td>\n",
       "      <td>analyst, saas, salesforce, training, consultin...</td>\n",
       "      <td>Founded in 1986, KPA is a leading provider of ...</td>\n",
       "      <td>REmote OK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                   company  \\\n",
       "0   1093192                  Ironclad   \n",
       "9   1093176         Warner Music Inc.   \n",
       "26  1093152                Fullscript   \n",
       "48  1093124  Utility Profit + Sunroom   \n",
       "64  1093098        500 WP Company LLC   \n",
       "86  1093062                       KPA   \n",
       "\n",
       "                                             position  \\\n",
       "0               Manager Engineering II Data Pipelines   \n",
       "9                     Staff Machine Learning Engineer   \n",
       "26                     Senior Data Analytics Engineer   \n",
       "48  Data and Business Intelligence Engineer Mid Le...   \n",
       "64            Computational Journalist Data Reporting   \n",
       "86                                       Data Analyst   \n",
       "\n",
       "                                                 tags  \\\n",
       "0   [manager, design, hr, docker, technical, softw...   \n",
       "9   [music, design, system, training, software, cl...   \n",
       "26  [design, python, technical, support, testing, ...   \n",
       "48  [design, saas, leader, operations, telecom, en...   \n",
       "64  [embedded, training, software, director, devop...   \n",
       "86  [analyst, saas, salesforce, training, consulti...   \n",
       "\n",
       "                          location  salary_min  salary_max  \\\n",
       "0                    San Francisco       67500       97500   \n",
       "9                                        60000       97500   \n",
       "26                                       67500      120000   \n",
       "48                                       70000      120000   \n",
       "64  DC-Washington-TWP Headquarters       57500       80000   \n",
       "86                          Remote       70000      115000   \n",
       "\n",
       "                 epoch                                        tags_string  \\\n",
       "0  2025-05-17 13:00:04  manager, design, hr, docker, technical, softwa...   \n",
       "9  2025-05-15 00:00:20  music, design, system, training, software, clo...   \n",
       "26 2025-05-10 21:00:03  design, python, technical, support, testing, g...   \n",
       "48 2025-05-07 08:00:15  design, saas, leader, operations, telecom, eng...   \n",
       "64 2025-05-03 00:00:03  embedded, training, software, director, devops...   \n",
       "86 2025-04-26 16:00:02  analyst, saas, salesforce, training, consultin...   \n",
       "\n",
       "                                          description     source  \n",
       "0   Ironclad is the #1 contract lifecycle manageme...  REmote OK  \n",
       "9   At Warner Music Group, weâre a global collec...  REmote OK  \n",
       "26  At Fullscript, weâre not just changing healt...  REmote OK  \n",
       "48  About Utility ProfitUtility Profit is transfor...  REmote OK  \n",
       "64  Application Instructions Please list all profe...  REmote OK  \n",
       "86  Founded in 1986, KPA is a leading provider of ...  REmote OK  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df = fetch_jobs_from_api(['remoteok'])\n",
    "#jobs_df = parse_jobs_to_structured_dataframe(raw_job_data, 'Remote OK')\n",
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90927a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate word cloud using tags on job postings\n",
    "tags = \", \".join(tag for tag in jobs_df.tags_string).lower()\n",
    "\n",
    "wordcloud = generate_wordcloud(tags)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfe11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "description = \" \".join(jobs_df.description).lower()\n",
    "\n",
    "word_list = word_tokenize(\" \".join(jobs_df.description).lower())\n",
    "filtered_words = [word for word in word_list if word == 'r' or (len(word) > 2 and word not in stopwords)]\n",
    "\n",
    "common_keywords = Counter(filtered_words).most_common(20) # Get the top 20 most frequent words\n",
    "\n",
    "print(\"\\nTop 20 most common keywords in job descriptions:\")\n",
    "for keyword, count in common_keywords:\n",
    "     print(f\"- {keyword}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation and creating DataFrames\n",
    "import requests  # For making HTTP requests to the API\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "def fetch_remoteok_jobs_from_api():\n",
    "    \"\"\"\n",
    "    Fetches job listings from the RemoteOK API.\n",
    "\n",
    "    The RemoteOK API returns a list. The first element is often a legal notice or API information,\n",
    "    so we skip it to get to the actual job listings.\n",
    "\n",
    "    Returns:\n",
    "        data: A list of job dictionaries if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    api_url = \"https://remoteok.com/api\"\n",
    "    print(f\"Attempting to fetch data from: {api_url}\")\n",
    "    try:\n",
    "        response = requests.get(api_url, timeout=10)  # Added timeout for robustness\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # The RemoteOK API returns a list. The first item is a \"legal notice\" or API info.\n",
    "        # Actual job listings start from the second item.\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            if data[0].get(\"legal\") is not None:\n",
    "                print(f\"Skipping the first element (meta-data/legal): {data[0].get('legal')}\")\n",
    "                return data[1:]  # Return the rest of the list\n",
    "            else:\n",
    "                # If the first element doesn't look like metadata, perhaps the API structure changed.\n",
    "                # For now, we'll assume it's all job data.\n",
    "                print(\"First element does not appear to be metadata. Processing all elements as data-jobs.\")\n",
    "                return data\n",
    "        elif isinstance(data, list) and len(data) == 0:\n",
    "            print(\"API returned an empty list of data-jobs.\")\n",
    "            return []\n",
    "        else:\n",
    "            print(f\"Unexpected API response format. Expected a list, got {type(data)}.\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Error: Request to {api_url} timed out.\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"Error: HTTP error occurred: {http_err} - Status Code: {response.status_code}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Error: An error occurred while fetching data from API: {req_err}\")\n",
    "        return None\n",
    "    except ValueError as json_err:  # Includes json.JSONDecodeError\n",
    "        print(f\"Error: Could not decode JSON response: {json_err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_jobicy_jobs_from_api():\n",
    "    \"\"\"\n",
    "    Fetches job listings from the Jobicy API.\n",
    "\n",
    "    The Jobicy API returns a list where the first element contains API information,\n",
    "    so we skip it to get to the actual job listings.\n",
    "\n",
    "    Returns:\n",
    "        data: A list of job dictionaries if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    api_url = \"https://jobicy.com/api/v2/remote-jobs\"\n",
    "    print(f\"Attempting to fetch data from: {api_url}\")\n",
    "    try:\n",
    "        response = requests.get(api_url, timeout=10)  # Added timeout for robustness\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # The RemoteOK API returns a list. The first item is contains API info.\n",
    "        # Actual job listings are in the second item.\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            if data[0].get(\"friendlyNotice\") is not None:\n",
    "                print(f\"Skipping the first element (meta-data/legal): {data[0].get('friendlyNotice')}\")\n",
    "                return data[1].get('data-jobs')  # Return the job list\n",
    "            else:\n",
    "                # If the first element doesn't look like metadata, perhaps the API structure changed.\n",
    "                # For now, we'll assume it's all job data.\n",
    "                print(\"First element does not appear to be metadata. Processing all elements as data-jobs.\")\n",
    "                return data\n",
    "        elif isinstance(data, list) and len(data) == 0:\n",
    "            print(\"API returned an empty list of data-jobs.\")\n",
    "            return []\n",
    "        else:\n",
    "            print(f\"Unexpected API response format. Expected a list, got {type(data)}.\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Error: Request to {api_url} timed out.\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"Error: HTTP error occurred: {http_err} - Status Code: {response.status_code}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Error: An error occurred while fetching data from API: {req_err}\")\n",
    "        return None\n",
    "    except ValueError as json_err:  # Includes json.JSONDecodeError\n",
    "        print(f\"Error: Could not decode JSON response: {json_err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_jobs_to_structured_dataframe(job_list):\n",
    "    \"\"\"\n",
    "    Parses a list of job dictionaries (from API) into a pandas DataFrame.\n",
    "    Selects relevant columns and performs basic data cleaning/transformation.\n",
    "\n",
    "    Args:\n",
    "        job_list (list): A list of dictionaries, where each dictionary represents a job.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing structured job data, or an empty DataFrame if input is invalid.\n",
    "    \"\"\"\n",
    "    if not job_list or not isinstance(job_list, list):\n",
    "        print(\"No job data provided or data is not in list format. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Normalizing {len(job_list)} job entries into a DataFrame...\")\n",
    "    # Use pandas.json_normalize to flatten the JSON structures.\n",
    "    df = pd.json_normalize(job_list)\n",
    "\n",
    "    # --- Data Cleaning and Transformation ---\n",
    "\n",
    "    # Define the columns we are interested in.\n",
    "    desired_columns = [\n",
    "        'id', 'company', 'position', 'tags', 'location', 'salary_min', 'salary_max'\n",
    "    ]\n",
    "    # Define the keywords to look for in job titles\n",
    "    keywords = 'analy|data|machine learning|intelligence'\n",
    "\n",
    "    # Select only the desired columns that are actually present in the DataFrame\n",
    "    # This makes the script more robust to changes in the API response\n",
    "    columns_to_select = [col for col in desired_columns if col in df.columns]\n",
    "\n",
    "    if not columns_to_select:\n",
    "        print(\"None of the desired columns were found in the API response. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_selected = df[columns_to_select][df['position'].str.contains(keywords, case=False) |\n",
    "                                        df['tags'].str.contains(keywords,\n",
    "                                                                case=False)].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Convert 'epoch' to datetime objects\n",
    "    if 'epoch' in df.columns:\n",
    "        # Ensure 'epoch' is numeric, coercing errors to NaT (Not a Time)\n",
    "        df_selected['epoch'] = pd.to_datetime(df['epoch'], unit='s', errors='coerce')\n",
    "\n",
    "    # Convert 'tags' list into a comma-separated string for easier use in SQL/CSV.\n",
    "    if 'tags' in df_selected.columns:\n",
    "        df_selected['tags_string'] = df_selected['tags'].apply(\n",
    "            lambda tags_list: ', '.join(tags_list) if isinstance(tags_list, list) and tags_list else None\n",
    "        )\n",
    "\n",
    "    # Clean up HTML and robot message from description\n",
    "    if 'description' in df.columns:\n",
    "        df_selected['description'] = df['description'].apply(\n",
    "            lambda html: bs(html, 'html.parser').get_text()\n",
    "        )\n",
    "        df_selected['description'] = df_selected['description'].str.replace(r'Please mention the word(.)*', \"\",\n",
    "                                                                            regex=True)\n",
    "\n",
    "    return df_selected\n",
    "\n",
    "\n",
    "def analyze_job_data(job_postings):\n",
    "    \"\"\"Performs basic analysis on the fetched job listings.\"\"\"\n",
    "\n",
    "    all_descriptions = \", \".join(job_postings.description).lower()\n",
    "    keywords = all_descriptions.split()\n",
    "    common_keywords = Counter(keywords).most_common(20) # Get the top 20 most frequent words\n",
    "\n",
    "    print(\"\\nTop 20 most common keywords in job descriptions:\")\n",
    "    for keyword, count in common_keywords:\n",
    "        if keyword not in STOPWORDS: # Basic stop word removal\n",
    "            print(f\"- {keyword}: {count}\")\n",
    "\n",
    "    # Generate word cloud using tags on job postings\n",
    "    tags = \", \".join(job_postings.tags_string).lower()\n",
    "\n",
    "    wordcloud = WordCloud().generate(tags)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"--- Starting RemoteOK Job Data Pipeline ---\")\n",
    "\n",
    "# Step 1: Fetch job data from the API\n",
    "raw_job_data = fetch_remoteok_jobs_from_api()\n",
    "raw_job_data2 = fetch_jobicy_jobs_from_api()\n",
    "\n",
    "if raw_job_data is None:\n",
    "    print(\"Failed to fetch job data from remoteok. Exiting pipeline.\")\n",
    "\n",
    "if not raw_job_data:\n",
    "    print(\"No job listings fetched from the API. Exiting pipeline.\")\n",
    "\n",
    "print(f\"Successfully fetched {len(raw_job_data)} raw job entries.\")\n",
    "\n",
    "# Step 2: Parse and transform data into a pandas DataFrame\n",
    "jobs_dataframe = parse_jobs_to_structured_dataframe(raw_job_data)\n",
    "\n",
    "if jobs_dataframe.empty:\n",
    "    print(\"DataFrame creation failed or resulted in an empty DataFrame. Exiting pipeline.\")\n",
    "\n",
    "print(\"\\n--- DataFrame Information ---\")\n",
    "jobs_dataframe.info()\n",
    "\n",
    "analyze_job_data(jobs_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'Name': 'Geeks', 1: [1, 2, 3, 4]} \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b0077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
